{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e8a2af",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df35a2-18ec-4944-afae-7e8aaf0db8b2",
   "metadata": {},
   "source": [
    "Método da descida do gradiente aplicado a funções quadráticas\n",
    "Tamanho do passo ótimo: $grad(f)^2/(grad(f)^TQgrad(f))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2129113-3523-4606-99ef-4a6bbe251820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   1, normGrad=5.656854, valFunc=0.666667\n",
      "iter   2, normGrad=1.885618, valFunc=0.074074\n",
      "iter   3, normGrad=0.628539, valFunc=0.008230\n",
      "iter   4, normGrad=0.209513, valFunc=0.000914\n",
      "iter   5, normGrad=0.069838, valFunc=0.000102\n",
      "iter   6, normGrad=0.023279, valFunc=0.000011\n",
      "iter   7, normGrad=0.007760, valFunc=0.000001\n",
      "iter   8, normGrad=0.002587, valFunc=0.000000\n",
      "iter   9, normGrad=0.000862, valFunc=0.000000\n",
      "iter  10, normGrad=0.000287, valFunc=0.000000\n",
      "iter  11, normGrad=0.000096, valFunc=0.000000\n",
      "iter  12, normGrad=0.000032, valFunc=0.000000\n",
      "iter  13, normGrad=0.000011, valFunc=0.000000\n",
      "iter  14, normGrad=0.000004, valFunc=0.000000\n",
      "iter  15, normGrad=0.000001, valFunc=0.000000\n",
      "\n",
      "Ponto mínimo encontrado: x = [ 1.39383439e-07 -6.96917194e-08]\n",
      "Valor mínimo da função: f(x) = 2.914161449771316e-14\n",
      "iter   1, normGrad=4.472136, valFunc=0.076923\n",
      "iter   2, normGrad=0.344010, valFunc=0.002959\n",
      "iter   3, normGrad=0.172005, valFunc=0.000114\n",
      "iter   4, normGrad=0.013231, valFunc=0.000004\n",
      "iter   5, normGrad=0.006616, valFunc=0.000000\n",
      "iter   6, normGrad=0.000509, valFunc=0.000000\n",
      "iter   7, normGrad=0.000254, valFunc=0.000000\n",
      "iter   8, normGrad=0.000020, valFunc=0.000000\n",
      "iter   9, normGrad=0.000010, valFunc=0.000000\n",
      "\n",
      "Ponto mínimo encontrado: x = [ 9.99999495e-01 -8.41653357e-07]\n",
      "Valor mínimo da função: f(x) = 3.6837199957062694e-13\n"
     ]
    }
   ],
   "source": [
    "# function [x, val_func] = MDMR_quadratic(Q, c, b, x0, epsilon)\n",
    "# x = x0;\n",
    "# k = 0;\n",
    "# grad = Q * x + c;\n",
    "# while (norm(grad) > epsilon)\n",
    "# k = k+1;\n",
    "# alpha = norm(grad)^2/(grad' * Q * grad);\n",
    "# x = x - alpha * grad;\n",
    "# val_func = 1/2 * x' * Q * x + c' * x + b;\n",
    "# fprintf('iter %3d, normGrad=%2.6f, valFunc=%2.6f\\n', k, norm(grad), valfunc);\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def MDMR_quadratic(Q, c, b, x0, epsilon):\n",
    "    \"\"\"\n",
    "    Método do Gradiente Descendente para minimizar uma função quadrática.\n",
    "\n",
    "    Args:\n",
    "        Q: Matriz Q da função quadrática (deve ser simétrica e definida positiva).\n",
    "        c: Vetor c da função quadrática.\n",
    "        b: Escalar b da função quadrática.\n",
    "        x0: Ponto inicial.\n",
    "        epsilon: Tolerância para a norma do gradiente.\n",
    "\n",
    "    Returns:\n",
    "        x: Ponto mínimo encontrado.\n",
    "        val_func: Valor da função no ponto mínimo.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0\n",
    "    k = 0\n",
    "    grad = Q @ x + c\n",
    "\n",
    "    while np.linalg.norm(grad) > epsilon:\n",
    "        k += 1\n",
    "        alpha = np.linalg.norm(grad)**2 / (grad.T @ Q @ grad)\n",
    "        x = x - alpha * grad\n",
    "        val_func = 0.5 * x.T @ Q @ x + c.T @ x + b\n",
    "        print(f\"iter {k:3d}, normGrad={np.linalg.norm(grad):2.6f}, valFunc={val_func:2.6f}\")\n",
    "        grad = Q @ x + c\n",
    "\n",
    "    return x, val_func\n",
    "\n",
    "# Exemplo: min f(x, y) = min (x^2 + 2*y^2), ponto inicial (2, 1) e tolerância 10^(-5)\n",
    "\n",
    "Q = np.array([[2, 0], [0, 4]])\n",
    "c = np.array([0, 0])\n",
    "b = 0\n",
    "x0 = np.array([2, 1])\n",
    "epsilon = 0.000001\n",
    "\n",
    "x_min, val_func_min = MDMR_quadratic(Q, c, b, x0, epsilon)\n",
    "\n",
    "print(f\"\\nPonto mínimo encontrado: x = {x_min}\")\n",
    "print(f\"Valor mínimo da função: f(x) = {val_func_min}\")\n",
    "\n",
    "# Aplicar o método da descida do gradiente com o passo otimizado ao problema quadrático: \n",
    "# min f(x_1, x_2) = (x_1 - 1)^2 + (x_1 - 1 - x_2)^2\n",
    "\n",
    "# Reescrevendo na forma f(x) = 1/2 * x^T * Q * x + c^T * x + b\n",
    "\n",
    "Q = np.array([[4, -2], [-2, 2]])\n",
    "c = np.array([-4, 2])\n",
    "b = 2\n",
    "x0 = np.array([0, 0])\n",
    "epsilon = 0.000001\n",
    "\n",
    "x_min, val_func_min = MDMR_quadratic(Q, c, b, x0, epsilon)\n",
    "\n",
    "print(f\"\\nPonto mínimo encontrado: x = {x_min}\")\n",
    "print(f\"Valor mínimo da função: f(x) = {val_func_min}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a2051",
   "metadata": {},
   "source": [
    "# Método de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123a0b7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sympy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Método de Newton\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Implementação do algoritmo\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbols, diff\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMN\u001b[39m(f, grad, Hess, x0, epsilon):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Método de Newton para encontrar o mínimo de uma função.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m        val_func: Valor da função no ponto mínimo.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sympy'"
     ]
    }
   ],
   "source": [
    "# Método de Newton\n",
    "# Implementação do algoritmo\n",
    "\n",
    "import numpy as np\n",
    "from sympy import symbols, diff\n",
    "\n",
    "def MN(f, grad, Hess, x0, epsilon):\n",
    "    \"\"\"\n",
    "    Método de Newton para encontrar o mínimo de uma função.\n",
    "\n",
    "    Args:\n",
    "        f: Função a ser minimizada (numérica).\n",
    "        grad: Função que calcula o gradiente da função f.\n",
    "        Hess: Função que calcula a matriz Hessiana da função f.\n",
    "        x0: Ponto inicial.\n",
    "        epsilon: Tolerância para a norma do gradiente.\n",
    "\n",
    "    Returns:\n",
    "        x: Ponto mínimo encontrado.\n",
    "        val_func: Valor da função no ponto mínimo.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    valG = grad(x)\n",
    "    valH = Hess(x)\n",
    "    i = 0\n",
    "    while (np.linalg.norm(valG) > epsilon) and (i < 10000):\n",
    "        i += 1\n",
    "        x = x - np.linalg.solve(valH, valG)\n",
    "        val_func = f(x)\n",
    "        print(f\"k = {i}, f(x) = {val_func:.10f}\")\n",
    "        valG = grad(x)\n",
    "        valH = Hess(x)\n",
    "    \n",
    "    if i == 10000:\n",
    "        print(\"O método não convergiu\")\n",
    "    \n",
    "    return x, val_func\n",
    "\n",
    "def generate_functions(f):\n",
    "    \"\"\"\n",
    "    Gera as funções de gradiente e Hessiana numericamente a partir da função simbólica f.\n",
    "\n",
    "    Args:\n",
    "        f: Função simbólica.\n",
    "\n",
    "    Returns:\n",
    "        gradient: Função que calcula o gradiente.\n",
    "        hessian: Função que calcula a Hessiana.\n",
    "    \"\"\"\n",
    "    # Definir variáveis simbólicas\n",
    "    x, y = symbols('x y')\n",
    "    \n",
    "    # Calcular o gradiente\n",
    "    grad_f = [diff(f, x), diff(f, y)]\n",
    "    \n",
    "    # Calcular a Hessiana\n",
    "    hess_f = [[diff(grad_f[0], x), diff(grad_f[0], y)],\n",
    "              [diff(grad_f[1], x), diff(grad_f[1], y)]]\n",
    "\n",
    "    # Funções para calcular o gradiente e a Hessiana numericamente\n",
    "    def gradient(point):\n",
    "        return np.array([grad_f[0].subs({x: point[0], y: point[1]}),\n",
    "                         grad_f[1].subs({x: point[0], y: point[1]})], dtype='float64')\n",
    "\n",
    "    def hessian(point):\n",
    "        return np.array([[hess_f[0][0].subs({x: point[0], y: point[1]}),\n",
    "                          hess_f[0][1].subs({x: point[0], y: point[1]})],\n",
    "                         [hess_f[1][0].subs({x: point[0], y: point[1]}),\n",
    "                          hess_f[1][1].subs({x: point[0], y: point[1]})]], dtype='float64')\n",
    "\n",
    "    return gradient, hessian\n",
    "\n",
    "# Exemplo 1: min 100x^4 + 0.01y^4\n",
    "x, y = symbols('x y')\n",
    "f = 100 * x**4 + 0.01 * y**4\n",
    "\n",
    "gradient, hessian = generate_functions(f)\n",
    "\n",
    "# Ponto inicial e tolerância\n",
    "x0 = np.array([1, 1])\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Aplicar o método de Newton\n",
    "def f_numeric(point):\n",
    "    return float(f.subs({x: point[0], y: point[1]}))\n",
    "\n",
    "x_min, val_func_min = MN(f_numeric, gradient, hessian, x0, epsilon)\n",
    "\n",
    "print(f\"\\nExemplo 1 - Ponto mínimo encontrado: x = {x_min}\")\n",
    "print(f\"Valor mínimo da função: f(x) = {val_func_min}\")\n",
    "\n",
    "# Exemplo 2: min sqrt(x^2 + 1) + sqrt(y^2 + 1)\n",
    "f = (x**2 + 1)**0.5 + (y**2 + 1)**0.5\n",
    "\n",
    "gradient, hessian = generate_functions(f)\n",
    "\n",
    "# Ponto inicial e tolerância\n",
    "x0 = np.array([1, 1])\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Aplicar o método de Newton\n",
    "x_min, val_func_min = MN(f_numeric, gradient, hessian, x0, epsilon)\n",
    "\n",
    "print(f\"\\nExemplo 2 - Ponto mínimo encontrado: x = {x_min}\")\n",
    "print(f\"Valor mínimo da função: f(x) = {val_func_min}\")\n",
    "\n",
    "# Exemplo 3: Outro exemplo com ponto inicial (0.5, 0.5)\n",
    "x0 = np.array([0.5, 0.5])\n",
    "x_min, val_func_min = MN(f_numeric, gradient, hessian, x0, epsilon)\n",
    "\n",
    "print(f\"\\nExemplo 3 - Ponto mínimo encontrado: x = {x_min}\")\n",
    "print(f\"Valor mínimo da função: f(x) = {val_func_min}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
